{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/ji-deza/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/ji-deza/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /Users/ji-deza/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/ji-deza/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/ji-\n",
      "[nltk_data]     deza/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "/Users/ji-deza/opt/anaconda3/envs/work/lib/python3.8/site-packages/sklearn/base.py:324: UserWarning: Trying to unpickle estimator DecisionTreeClassifier from version 1.0.2 when using version 1.0.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/ji-deza/opt/anaconda3/envs/work/lib/python3.8/site-packages/sklearn/base.py:324: UserWarning: Trying to unpickle estimator RandomForestClassifier from version 1.0.2 when using version 1.0.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "max_features = 6000\n",
    "lst_stopwords = joblib.load('stopwords.joblib')\n",
    "accepted_words = joblib.load('accepted_words.joblib')\n",
    "icms_dct = joblib.load('icms1_dictionnary.joblib') \n",
    "Cat = joblib.load('Categories_ML.joblib')\n",
    "clf = joblib.load('Random_Forests_trained_model.joblib.gz');\n",
    "\n",
    "def utils_preprocess_text(text, flg_stemm=False, flg_lemm=True, lst_stopwords=None):\n",
    "    ## clean (convert to lowercase and remove punctuations and characters and then strip)\n",
    "    text = re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\n",
    "    #remove if only two characters\n",
    "    text_big = re.sub(r'\\W*\\b\\w{1,2}\\b', '', text) \n",
    "          \n",
    "    ## Tokenize (convert from string to list)\n",
    "    lst_text = text_big.split()\n",
    "    ## remove Stopwords\n",
    "    if lst_stopwords is not None:\n",
    "        lst_text = [word for word in lst_text if word not in lst_stopwords]\n",
    "    \n",
    "    ## Stemming (remove -ing, -ly, ...)\n",
    "    if flg_stemm == True:\n",
    "        ps = nltk.stem.porter.PorterStemmer()\n",
    "        lst_text = [ps.stem(word) for word in lst_text]\n",
    "\n",
    "    ## Lemmatisation (convert the word into root word)\n",
    "    if flg_lemm == True:\n",
    "        lem = WordNetLemmatizer()\n",
    "        lst_text = [lem.lemmatize(word) for word in lst_text]\n",
    "        ## removing tags\n",
    "        ## removing digits\n",
    "    ## back to string from list\n",
    "    text = \" \".join(lst_text)\n",
    "    return text\n",
    "\n",
    "def clean_text_data(df,col):\n",
    "    # Separate data\n",
    "    desc_lower = df[col]\n",
    "    # Remove text before \"|\" character\n",
    "    desc_split = desc_lower.str.split(\"|\")\n",
    "    desc_strip = desc_split.apply(lambda x: x[1] if len(x) > 1 else x[0])\n",
    "    # Removing digits and words containing digits\n",
    "    desc_nodigits = desc_strip.apply(lambda x: re.sub(\"\\w*\\d\\w*\", \"\", x))\n",
    "    # Removing punctuation\n",
    "    desc_nopunc = desc_nodigits.apply(lambda x: re.sub(r\"[^\\w\\s]\", \"\", x))\n",
    "    # Removing additional whitespace\n",
    "    desc_clean = desc_nopunc.apply(lambda x: re.sub(' +', ' ', x))\n",
    "    return desc_clean\n",
    "\n",
    "def goodCode(name,code,desc):\n",
    "    A = {'success' : 'true' , \"Message\" : name ,'ICMS' : code ,\"Description\" : desc}\n",
    "    R = A['ICMS'].split('.')\n",
    "    D = [x.strip() for x  in A['Description'].split('\\\\')]\n",
    "    A['R2'] = R[0]\n",
    "    A['R3'] = R[1]\n",
    "    A['R4'] = R[2]\n",
    "\n",
    "    A['Desc2'] = D[0]\n",
    "    A['Desc3'] = D[1]\n",
    "    A['Desc4'] = D[2]\n",
    "    return A\n",
    "\n",
    "def predict(names):\n",
    "    if type(names) == str:\n",
    "        names = [names]\n",
    "    A = pd.DataFrame()\n",
    "    A['comment_list'] = names\n",
    "    A['comment_list'] = A.comment_list.apply(lambda x: utils_preprocess_text(x, flg_stemm=False, flg_lemm=True, lst_stopwords=lst_stopwords))\n",
    "    A['comment_list_new'] = clean_text_data(A,'comment_list')\n",
    "    vectorizer = CountVectorizer(max_features=max_features,strip_accents='ascii',vocabulary=accepted_words)\n",
    "    X = vectorizer.fit_transform(A['comment_list_new']).toarray()\n",
    "    #X = X.astype(dtype=bool).astype(dtype=int)\n",
    "\n",
    "    y_pred = clf.predict(X)\n",
    "    codes = [Cat[x] for x in y_pred]\n",
    "    descs =[icms_dct[c] for c in codes]\n",
    "    answer = [goodCode(names[i],codes[i],descs[i]) for i,_ in enumerate(names)]\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'success': 'true',\n",
       " 'Message': 'Clear site of all signs and equipment (including diversion routes)',\n",
       " 'ICMS': '1.08.060',\n",
       " 'Description': \"Capital Construction Costs \\\\Preliminaries | Constructor's site overheads | general requirements\\\\ Other temporary facilities and services\",\n",
       " 'R2': '1',\n",
       " 'R3': '08',\n",
       " 'R4': '060',\n",
       " 'Desc2': 'Capital Construction Costs',\n",
       " 'Desc3': \"Preliminaries | Constructor's site overheads | general requirements\",\n",
       " 'Desc4': 'Other temporary facilities and services'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict('Clear site of all signs and equipment (including diversion routes)')[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.27"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv('renewals_sample1.csv')\n",
    "A = predict(test.Description.to_list())\n",
    "test['ICMS_ML']=[x['ICMS'] for x in A]\n",
    "test['A']=[x['Desc2'] for x in A]\n",
    "test['B']=[x['Desc3'] for x in A]\n",
    "test['C']=[x['Desc4'] for x in A]\n",
    "\n",
    "test[test['ICMS'] != test['ICMS_ML']].shape[0] / test.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_csv('discrepancy.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Â Fix an error in the icms dict\n",
    "import joblib\n",
    "import pandas as pd\n",
    "tmp = {}\n",
    "tmp['Description'] = joblib.load('icms1_dictionnary.joblib')\n",
    "tmp = pd.DataFrame(data=tmp)\n",
    "tmp['Description'].to_csv('Desc.csv')\n",
    "tmp[['A','B','C']] = tmp.Description.str.split('\\\\',expand=True)\n",
    "tmp.loc[tmp.B.str.contains(\"Preliminaries \\| Cons\") ,'B'] =  \"Preliminaries | Constructor\\'s site overheads | general requirements\"\n",
    "tmp.Description = tmp.A + '\\\\' + tmp.B + '\\\\' + tmp.C\n",
    "joblib.dump(tmp['Description'].to_dict(),'icms1_dictionnary.joblib')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "324d272a21728490c3b6d9ee1aa416604d329fe974c5eb86d19838724c3e805f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.2 ('work')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
